{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e316eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974d5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def create_mask_from_json(json_file, output_dir=None, save=True):\n",
    "    \"\"\"\n",
    "    Creates a full-resolution binary mask from a LabelMe JSON file.\n",
    "    Background is Black (0), Polygons are White (255).\n",
    "    \"\"\"\n",
    "    # 1. Load the JSON data\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. Get Image Dimensions\n",
    "    # LabelMe JSONs usually store height/width. \n",
    "    # If missing, we default to opening the image file to check.\n",
    "    height = data.get('imageHeight')\n",
    "    width = data.get('imageWidth')\n",
    "\n",
    "    if not height or not width:\n",
    "        # Fallback: Try to find the image to get size\n",
    "        image_path = Path(json_file).parent / data.get('imagePath', '')\n",
    "        if image_path.exists():\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "        else:\n",
    "            raise ValueError(f\"Could not determine dimensions for {json_file}\")\n",
    "\n",
    "    # 3. Create a blank Black image (Mode 'L' = 8-bit grayscale)\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # 4. Draw all polygons in White\n",
    "    for shape in data.get('shapes', []):\n",
    "        if shape.get('shape_type') == 'polygon':\n",
    "            # Convert points to a list of tuples [(x,y), (x,y)...]\n",
    "            points = [tuple(p) for p in shape['points']]\n",
    "            \n",
    "            # Fill the polygon with 255 (White)\n",
    "            draw.polygon(points, outline=255, fill=255)\n",
    "\n",
    "    # 5. Save the mask\n",
    "    if save:\n",
    "        if output_dir is None:\n",
    "            # Default: save in a 'masks' folder next to the json\n",
    "            output_dir = Path(json_file).parent / 'masks'\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save as PNG (Important! JPG compresses and ruins masks)\n",
    "        out_name = f\"{Path(json_file).stem}_mask.png\"\n",
    "        mask.save(Path(output_dir) / out_name)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2316a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask created!\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your images directory\n",
    "images_dir = Path('data')\n",
    "\n",
    "\n",
    "# Example: Process a single file\n",
    "example_json = images_dir / '20200713_1207268869_plant1041_rgb_trigger010.json'\n",
    "\n",
    "if example_json.exists():\n",
    "    # Create mask for this file\n",
    "    mask = create_mask_from_json(\n",
    "        example_json,\n",
    "        output_dir=images_dir / 'masks',\n",
    "        save=False\n",
    "    )\n",
    "    \n",
    "    print(\"Mask created!\")\n",
    "    \n",
    "    # Display the mask\n",
    "    if mask is not None:\n",
    "        mask.show()\n",
    "else:\n",
    "    print(f\"File not found: {example_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27088fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]An error occurred while trying to fetch C:\\Users\\ShanO\\.cache\\huggingface\\hub\\models--Fantasy-Studio--Paint-by-Example\\snapshots\\351e6427d8c28a3b24f7c751d43eb4b6735127f7\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\ShanO\\.cache\\huggingface\\hub\\models--Fantasy-Studio--Paint-by-Example\\snapshots\\351e6427d8c28a3b24f7c751d43eb4b6735127f7\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  40%|████      | 2/5 [00:00<00:00,  3.07it/s]You are using a model of type clip_vision_model to instantiate a model of type clip. This is not supported for all configurations of models and can yield errors.\n",
      "Loading pipeline components...:  60%|██████    | 3/5 [00:02<00:01,  1.10it/s]An error occurred while trying to fetch C:\\Users\\ShanO\\.cache\\huggingface\\hub\\models--Fantasy-Studio--Paint-by-Example\\snapshots\\351e6427d8c28a3b24f7c751d43eb4b6735127f7\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\ShanO\\.cache\\huggingface\\hub\\models--Fantasy-Studio--Paint-by-Example\\snapshots\\351e6427d8c28a3b24f7c751d43eb4b6735127f7\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:05<00:00,  1.17s/it]\n",
      "The PaintByExamplePipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    }
   ],
   "source": [
    "# 1. Load the specific \"Paint By Example\" model\n",
    "# This model is trained specifically to copy the STYLE of the reference into the MASK\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"Fantasy-Studio/Paint-by-Example\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffcab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load and preprocess your images\n",
    "# Paint-by-Example works best with 512x512 images\n",
    "TARGET_SIZE = 512\n",
    "\n",
    "def preprocess_image(img, target_size=TARGET_SIZE):\n",
    "    \"\"\"Convert to RGB and resize to target size (maintains aspect ratio, center crops)\"\"\"\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    \n",
    "    # Calculate resize dimensions maintaining aspect ratio\n",
    "    width, height = img.size\n",
    "    aspect_ratio = width / height\n",
    "    \n",
    "    if width > height:\n",
    "        new_width = target_size\n",
    "        new_height = int(target_size / aspect_ratio)\n",
    "    else:\n",
    "        new_height = target_size\n",
    "        new_width = int(target_size * aspect_ratio)\n",
    "    \n",
    "    # Resize\n",
    "    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Center crop to exact target size\n",
    "    left = (new_width - target_size) // 2\n",
    "    top = (new_height - target_size) // 2\n",
    "    right = left + target_size\n",
    "    bottom = top + target_size\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_mask(mask, target_size=TARGET_SIZE):\n",
    "    \"\"\"Resize mask to target size (grayscale, maintains aspect ratio, center crops)\"\"\"\n",
    "    if mask.mode != 'L':\n",
    "        mask = mask.convert('L')\n",
    "    \n",
    "    # Calculate resize dimensions maintaining aspect ratio\n",
    "    width, height = mask.size\n",
    "    aspect_ratio = width / height\n",
    "    \n",
    "    if width > height:\n",
    "        new_width = target_size\n",
    "        new_height = int(target_size / aspect_ratio)\n",
    "    else:\n",
    "        new_height = target_size\n",
    "        new_width = int(target_size * aspect_ratio)\n",
    "    \n",
    "    # Resize\n",
    "    mask = mask.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Center crop to exact target size\n",
    "    left = (new_width - target_size) // 2\n",
    "    top = (new_height - target_size) // 2\n",
    "    right = left + target_size\n",
    "    bottom = top + target_size\n",
    "    mask = mask.crop((left, top, right, bottom))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Load images\n",
    "image = Image.open(\"data/20200713_1207268869_plant1041_rgb_trigger010.png\")\n",
    "mask_image = mask\n",
    "example_image = Image.open(\"reference_diseases/Figure-1_poly1.png\")\n",
    "\n",
    "# Preprocess all images to same size\n",
    "image = preprocess_image(image, TARGET_SIZE)\n",
    "mask_image = preprocess_mask(mask_image, TARGET_SIZE)\n",
    "example_image = preprocess_image(example_image, TARGET_SIZE)\n",
    "\n",
    "# Verify sizes match\n",
    "print(f\"Image size: {image.size}, mode: {image.mode}\")\n",
    "print(f\"Mask size: {mask_image.size}, mode: {mask_image.mode}\")\n",
    "print(f\"Example size: {example_image.size}, mode: {example_image.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c9791",
   "metadata": {},
   "source": [
    "## Tips for Better Results:\n",
    "\n",
    "1. **Reference Image Quality**: Make sure your reference disease image is clear and shows the disease pattern well\n",
    "2. **Mask Precision**: The mask should accurately outline where you want the disease to appear\n",
    "3. **Guidance Scale**: \n",
    "   - Lower (3.0-4.0): More creative, less faithful to reference\n",
    "   - Higher (6.0-7.0): More faithful to reference, less blending\n",
    "4. **Inference Steps**: More steps (75-100) = better quality but slower\n",
    "5. **Try Different Seeds**: Change the seed value to get different variations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:17<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# 3. Run generation with optimized parameters\n",
    "# Paint-by-Example works better with more inference steps and proper guidance\n",
    "result = pipe(\n",
    "    image=image,\n",
    "    mask_image=mask_image,\n",
    "    example_image=example_image,\n",
    "    num_inference_steps=50,  # More steps = better quality (default is 50, but can increase to 100)\n",
    "    guidance_scale=5.0,  # Higher guidance = stronger adherence to example (try 3.0-7.0)\n",
    "    generator=torch.Generator().manual_seed(42),  # For reproducibility\n",
    ").images[0]\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"synthetic_data\", exist_ok=True)\n",
    "result.save(\"synthetic_data/output.png\")\n",
    "print(\"Result saved to synthetic_data/output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Experiment with different parameters\n",
    "# Uncomment and modify to try different settings\n",
    "\n",
    "# result_v2 = pipe(\n",
    "#     image=image,\n",
    "#     mask_image=mask_image,\n",
    "#     example_image=example_image,\n",
    "#     num_inference_steps=75,  # More steps for better quality\n",
    "#     guidance_scale=6.0,  # Higher for stronger reference adherence\n",
    "#     generator=torch.Generator().manual_seed(123),  # Different seed for variation\n",
    "# ).images[0]\n",
    "# \n",
    "# result_v2.save(\"synthetic_data/output_v2.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpainting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
